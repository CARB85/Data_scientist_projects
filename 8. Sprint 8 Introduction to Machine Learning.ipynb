{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description #\n",
    "\n",
    "* **Objective**: Develop a classification model to recommend one of Megaline's new plans (Smart or Ultra) based on customer behavior.\n",
    "\n",
    "* **Data**: The dataset contains monthly user information, including calls, minutes, messages, MB used, and the current plan (Smart or Ultra).\n",
    "\n",
    "# Table of Contents <a id='back'></a>\n",
    "\n",
    "* [Introduction](#intro)\n",
    "* [1. Data Processing](#data_review)\n",
    "    * [1.1 Data Loading and Initial Exploration](#activity)\n",
    "    * [1.2 Data Segmentation](#activity)\n",
    "* [2. Classification Model Comparison: Performance Evaluation](#performance_evaluation)\n",
    "* [3. Hyperparameter Optimization of RandomForestClassifier with GridSearchCV](#hyperparameter_optimization)\n",
    "* [4. Training and Evaluation of RandomForest Model with Optimal Hyperparameters](#training_and_evaluation)\n",
    "* [5. Next Steps](#next_steps)\n",
    "    * [5.1 Sanity Check](#activity)\n",
    "    * [5.2 Class Balancing](#activity)\n",
    "    * [5.3 Feature Engineering](#activity)\n",
    "* [6. Results Analysis](#analysis_of_results)\n",
    "* [7. Final Steps](#final_steps)\n",
    "* [General Conclusion](#end)\n",
    "\n",
    "## Introduction <a id='intro'></a>\n",
    "\n",
    "The mobile company Megaline is unsatisfied with seeing many of its customers still using legacy plans. They want to develop a model that can analyze customer behavior and recommend one of Megaline's new plans: Smart or Ultra.\n",
    "\n",
    "You have access to the behavior data of subscribers who have already switched to the new plans (from the Data Statistical Analysis Sprint project). For this classification task, you need to create a model that selects the correct plan. Since you've already processed the data, you can jump straight into creating the model.\n",
    "\n",
    "Develop a model with the highest accuracy possible. In this project, the accuracy threshold is 0.75. Use the dataset to validate the accuracy.\n",
    "\n",
    "## 1. Data Processing <a id='data_review'></a>\n",
    "\n",
    "Each observation in the dataset contains monthly behavior information about a user. The provided information is as follows:\n",
    "\n",
    "* calls — number of calls\n",
    "* minutes — total call duration in minutes\n",
    "* messages — number of text messages\n",
    "* mb_used — Internet traffic used in MB\n",
    "* is_ultra — plan for the current month (Ultra - 1, Smart - 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report, confusion_matrix\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "# Load all the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "#from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Loading and Initial Exploration ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calls</th>\n",
       "      <th>minutes</th>\n",
       "      <th>messages</th>\n",
       "      <th>mb_used</th>\n",
       "      <th>is_ultra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.0</td>\n",
       "      <td>311.90</td>\n",
       "      <td>83.0</td>\n",
       "      <td>19915.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85.0</td>\n",
       "      <td>516.75</td>\n",
       "      <td>56.0</td>\n",
       "      <td>22696.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>77.0</td>\n",
       "      <td>467.66</td>\n",
       "      <td>86.0</td>\n",
       "      <td>21060.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106.0</td>\n",
       "      <td>745.53</td>\n",
       "      <td>81.0</td>\n",
       "      <td>8437.39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66.0</td>\n",
       "      <td>418.74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14502.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   calls  minutes  messages   mb_used  is_ultra\n",
       "0   40.0   311.90      83.0  19915.42         0\n",
       "1   85.0   516.75      56.0  22696.96         0\n",
       "2   77.0   467.66      86.0  21060.45         0\n",
       "3  106.0   745.53      81.0   8437.39         1\n",
       "4   66.0   418.74       1.0  14502.75         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data from the file\n",
    "file_path = '/datasets/users_behavior.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Examine the first rows of the dataframe\n",
    "data.info()\n",
    "print('\\n')\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calls       0\n",
      "minutes     0\n",
      "messages    0\n",
      "mb_used     0\n",
      "is_ultra    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This DataFrame has the following structure:\n",
    "\n",
    "* Number of rows: 3214\n",
    "* Number of columns: 5\n",
    "\n",
    "    * Columns and their data types:\n",
    "    - calls: 3214 non-null values, type float64\n",
    "    - minutes: 3214 non-null values, type float64\n",
    "    - messages: 3214 non-null values, type float64\n",
    "    - mb_used: 3214 non-null values, type float64\n",
    "    - is_ultra: 3214 non-null values, type int64\n",
    "\n",
    "    * Memory usage:\n",
    "    - Memory used: 125.7 KB\n",
    "\n",
    "    * Description:\n",
    "    This DataFrame contains data on mobile service usage, with the following columns:\n",
    "\n",
    "    - calls: Number of calls made.\n",
    "    - minutes: Total minutes of calls made.\n",
    "    - messages: Number of messages sent.\n",
    "    - mb_used: Megabytes of data used.\n",
    "    - is_ultra: Indicates if the user has an \"Ultra\" plan (1) or not (0).\n",
    "\n",
    "All columns have complete data, with no missing values, and the data types are suitable for quantitative analysis.\n",
    "\n",
    "### 1.2 Data segmentation ###\n",
    "\n",
    "We will segment the data into training, validation, and test sets. We will use 60% of the data for training, 20% for validation, and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1928, 4), (643, 4), (643, 4))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the features (X) and the target variable (y)\n",
    "X = data.drop(columns=['is_ultra'])\n",
    "y = data['is_ultra']\n",
    "\n",
    "# Split the data into training, validation, and test sets (60%, 20%, 20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Show the shape of the datasets\n",
    "(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations ####\n",
    "\n",
    "Data Split:\n",
    "\n",
    "* The data has been divided into three sets: training, validation, and test.\n",
    "* The split ratio is 60% for training, 20% for validation, and 20% for testing.\n",
    "* This ratio ensures that the model has enough data to train, validate, and test its performance in a balanced manner.\n",
    "\n",
    "Shape of the Data Sets:\n",
    "\n",
    "* The training set (X_train) contains 1928 samples and 4 features.\n",
    "* The validation set (X_val) contains 643 samples and 4 features.\n",
    "* The test set (X_test) also contains 643 samples and 4 features.\n",
    "* The shape of the data sets indicates that the split was done correctly according to the specified proportions.\n",
    "\n",
    "Use of Stratify:\n",
    "\n",
    "* The stratify=y parameter in train_test_split ensures that the distribution of the target variable (is_ultra) is the same across the training, validation, and test sets. This is important to ensure that each data set is representative of the complete set, especially if the target variable is imbalanced.\n",
    "\n",
    "Consistency in Features:\n",
    "\n",
    "* All data sets (training, validation, and test) have the same number of features (4). This is crucial to ensure that the model can be trained and evaluated correctly across all data sets.\n",
    "\n",
    "Data Set Size:\n",
    "\n",
    "* The total size of the data set is 1928 (training) + 643 (validation) + 643 (test) = 3214 samples. This is consistent with the original amount of data in the set, which has 3214 samples.\n",
    "\n",
    "## 2. Model Comparison: Performance Evaluation <a id='performance_evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': 0.7045101088646968,\n",
       " 'Random Forest': 0.8009331259720062,\n",
       " 'SVM': 0.7527216174183515}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate each model and save the results\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    results[model_name] = accuracy\n",
    "\n",
    "# Show the results\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations ####\n",
    "\n",
    "Model Evaluation:\n",
    "\n",
    "* Three different classification models were evaluated: Logistic Regression, Random Forest, and SVM.\n",
    "* Each model was trained using the training set (X_train, y_train) and then evaluated on the validation set (X_val).\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "The accuracy results for each model are as follows:\n",
    "\n",
    "* Logistic Regression: 0.7496\n",
    "* Random Forest: 0.8009\n",
    "* SVM: 0.7527\n",
    "\n",
    "These values represent the proportion of correct predictions made by each model on the validation set.\n",
    "\n",
    "Comparative Performance:\n",
    "\n",
    "* Random Forest is the best-performing model, with an accuracy of 0.8009, outperforming both Logistic Regression and SVM.\n",
    "* SVM performs slightly better than Logistic Regression, with an accuracy of 0.7527 compared to 0.7496.\n",
    "* Logistic Regression has the lowest accuracy among the three models, although the difference with SVM is not very large.\n",
    "\n",
    "Implications:\n",
    "\n",
    "* Since Random Forest has the highest accuracy, it could be considered the most promising model for this specific problem.\n",
    "* The difference in accuracy between the models is not extremely large, suggesting that all models perform relatively comparably on this dataset.\n",
    "* However, the final model selection should not be based solely on accuracy. Other factors such as model interpretability, training time, and ability to handle imbalanced data should also be considered.\n",
    "\n",
    "Random Forest shows the best performance in terms of accuracy on the validation set, followed closely by SVM, and then Logistic Regression.\n",
    "\n",
    "## 3. Hyperparameter Optimization of RandomForestClassifier with GridSearchCV <a id='hyperparameter_optimization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 10,\n",
       "  'min_samples_leaf': 2,\n",
       "  'min_samples_split': 5,\n",
       "  'n_estimators': 200},\n",
       " 0.8101626429848404)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the hyperparameters to investigate\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Set up the GridSearchCV\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "(best_params, best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations ####\n",
    "\n",
    "Hyperparameter Optimization:\n",
    "\n",
    "GridSearchCV was used to find the best hyperparameters for a RandomForestClassifier model.\n",
    "\n",
    "The hyperparameters investigated include:\n",
    "* n_estimators: Number of trees in the forest (50, 100, 200).\n",
    "* max_depth: Maximum depth of the trees (None, 10, 20, 30).\n",
    "* min_samples_split: Minimum number of samples required to split a node (2, 5, 10).\n",
    "* min_samples_leaf: Minimum number of samples required at a leaf (1, 2, 4).\n",
    "\n",
    "GridSearchCV Setup:\n",
    "\n",
    "* 3-fold cross-validation was used (cv=3).\n",
    "* The search process was parallelized using all available cores (n_jobs=-1).\n",
    "* Accuracy was used as the metric to evaluate the models (scoring='accuracy').\n",
    "\n",
    "Hyperparameter Search Results:\n",
    "\n",
    "The best hyperparameters found are:\n",
    "* max_depth: 10\n",
    "* min_samples_leaf: 2\n",
    "* min_samples_split: 5\n",
    "* n_estimators: 200\n",
    "\n",
    "These values represent the combination of hyperparameters that maximizes the model's accuracy on the validation set during cross-validation.\n",
    "\n",
    "Best Score:\n",
    "\n",
    "* The best accuracy obtained with the optimal hyperparameters is approximately 0.8102.\n",
    "\n",
    "This score reflects the proportion of correct predictions made by the optimized model on the validation set during cross-validation.\n",
    "\n",
    "Implications:\n",
    "\n",
    "* The combination of optimized hyperparameters suggests that a RandomForestClassifier with 200 trees, a maximum depth of 10, a minimum of 2 samples in a leaf, and a minimum of 5 samples to split a node, provides the best performance in terms of accuracy.\n",
    "* The accuracy of approximately 0.8102 is a potential improvement over the previously obtained values, indicating that hyperparameter optimization has been beneficial for the model's performance.\n",
    "\n",
    "Using GridSearchCV has allowed identifying an optimal combination of hyperparameters for the RandomForestClassifier model, resulting in an improvement in the model's accuracy on the validation set. This optimization is crucial for enhancing model performance in classification tasks.\n",
    "\n",
    "## 4. Training and Evaluation of the RandomForest Model with Optimal Hyperparameters <a id='training_and_evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.807153965785381\n",
      "Confusion Matrix:\n",
      " [[411  35]\n",
      " [ 89 108]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87       446\n",
      "           1       0.76      0.55      0.64       197\n",
      "\n",
      "    accuracy                           0.81       643\n",
      "   macro avg       0.79      0.73      0.75       643\n",
      "weighted avg       0.80      0.81      0.80       643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the best hyperparameters\n",
    "best_rf_model = RandomForestClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=5,\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "class_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations ####\n",
    "\n",
    "Model accuracy on the test set:\n",
    "\n",
    "The accuracy of the RandomForestClassifier model on the test set is 0.8072, meaning that the model correctly predicts 80.72% of the samples.\n",
    "\n",
    "* Confusion Matrix:\n",
    "\n",
    "    The confusion matrix shows the following results:\n",
    "    * True negatives (0 predicted as 0): 411\n",
    "    * False positives (0 predicted as 1): 35\n",
    "    * False negatives (1 predicted as 0): 89\n",
    "    * True positives (1 predicted as 1): 108\n",
    "\n",
    "This indicates that the model is more successful at predicting class 0 than class 1.\n",
    "\n",
    "* Classification Report:\n",
    "\n",
    "    * Precision:\n",
    "        * Class 0: 0.82\n",
    "        * Class 1: 0.76\n",
    "\n",
    "    * Recall:\n",
    "        * Class 0: 0.92\n",
    "        * Class 1: 0.55\n",
    "\n",
    "    * F1-score:\n",
    "        * Class 0: 0.87\n",
    "        * Class 1: 0.64\n",
    "\n",
    "    * Support:\n",
    "        * Class 0: 446\n",
    "        * Class 1: 197\n",
    "\n",
    "Interpretation of the metrics:\n",
    "\n",
    "* Precision: The precision for class 0 is higher than for class 1, indicating that when the model predicts class 0, it is correct 82% of the time, while for class 1, it is correct 76% of the time.\n",
    "* Recall: The recall for class 0 is 0.92, meaning that the model correctly identifies 92% of class 0 cases. However, for class 1, the recall is only 0.55, indicating that the model correctly identifies only 55% of class 1 cases.\n",
    "* F1-score: The F1-score, which is the harmonic mean of precision and recall, is 0.87 for class 0 and 0.64 for class 1, suggesting that the model performs significantly better on class 0.\n",
    "\n",
    "Overall Performance:\n",
    "\n",
    "* The overall accuracy of the model is good, with a value of 0.8072, indicating that the model is fairly accurate overall.\n",
    "* The macro avg and weighted avg show that the model's performance is balanced, though the lower recall for class 1 drags the average.\n",
    "\n",
    "The RandomForestClassifier model trained with the best hyperparameters shows good overall performance with an accuracy of 80.72%. However, it performs better in predicting class 0 compared to class 1, as indicated by the precision, recall, and F1-score metrics. This suggests that there might be an imbalance in the classes or that the model needs further tuning to improve performance on class 1.\n",
    "\n",
    "### Interpretation of Results:\n",
    "\n",
    "- **Accuracy**: The model's accuracy is 0.81, which is above the required threshold of 0.75.\n",
    "- **Confusion Matrix**:\n",
    "    - The model correctly classified 411 cases as Smart (0) and 108 cases as Ultra (1).\n",
    "    - There were 35 false positives (classified as Ultra when they were actually Smart) and 89 false negatives (classified as Smart when they were actually Ultra).\n",
    "- **Classification Report**:\n",
    "    - The precision for the Smart (0) class is high (0.82), indicating that the model is quite reliable at correctly classifying this class.\n",
    "    - The precision for the Ultra (1) class is lower (0.76), suggesting that the model has more difficulty identifying Ultra cases correctly.\n",
    "    - The recall for the Ultra (1) class is 0.55, indicating that the model is missing some Ultra cases.\n",
    "\n",
    "## 5. Next steps: <a id='next_steps'></a>\n",
    "\n",
    "1. **Sanity Check**: To perform a sanity check, you can test the model with random or very simple data to ensure it is not overfitting.\n",
    "2. **Additional Improvements**:\n",
    "    - **Class Balancing**: Given the significant difference between the classes in terms of recall, class balancing techniques like oversampling the minority class (Ultra) or undersampling the majority class (Smart) could be tried.\n",
    "    - **Other Modeling Techniques**: Try additional models or model combinations (ensembles) to see if performance can be further improved.\n",
    "    - **Feature Engineering**: Create new features from existing ones that may help improve the model's predictive power.\n",
    "\n",
    "### 5.1. Sanity Check ###\n",
    "\n",
    "A simple sanity check could involve testing the model with random data to ensure it is not achieving high accuracy by chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Data Test Accuracy: 0.30637636080870917\n"
     ]
    }
   ],
   "source": [
    "# Generate random data with the same shape as X_test\n",
    "X_test_random = np.random.rand(X_test.shape[0], X_test.shape[1])\n",
    "\n",
    "# Predict using the trained model\n",
    "y_test_random_pred = best_rf_model.predict(X_test_random)\n",
    "\n",
    "# Evaluate the performance on random data\n",
    "random_accuracy = accuracy_score(y_test, y_test_random_pred)\n",
    "\n",
    "print(f\"Random Data Test Accuracy: {random_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations ####\n",
    "\n",
    "Random Data Generation:\n",
    "\n",
    "* Random data was generated with the same shape as X_test using np.random.rand. This creates a random test dataset that has the same number of samples and features as the original test set.\n",
    "\n",
    "Predictions with Random Data:\n",
    "\n",
    "* The previously trained RandomForestClassifier model (best_rf_model) was used to make predictions on this random dataset (X_test_random).\n",
    "\n",
    "Accuracy on Random Data:\n",
    "\n",
    "* The accuracy of the model on random data is 0.3064, meaning the model correctly predicts only 30.64% of the random samples.\n",
    "\n",
    "Performance Interpretation:\n",
    "\n",
    "* Low Accuracy: The low accuracy on random data is expected and normal. Random data does not contain any structure or patterns that the model can recognize or learn, resulting in poor performance.\n",
    "* Comparison with Performance on Real Data: Compared to the accuracy of approximately 80.72% on the real test set, the accuracy on random data (30.64%) is significantly lower. This confirms that the model performs well on structured and known data, but poorly on random data.\n",
    "\n",
    "Learning Confirmation:\n",
    "\n",
    "* This experiment demonstrates that the model has learned specific patterns from the training data. If the model had shown high accuracy on random data, it would have indicated that the model was merely \"guessing\" rather than learning.\n",
    "\n",
    "The RandomForestClassifier model shows a low accuracy (30.64%) on random data, which is consistent with expectations, as random data contains no useful patterns for the model. This observation confirms that the model has effectively learned patterns from the training data and does not perform well on unstructured data.\n",
    "\n",
    "### 5.2. Class Balancing ###\n",
    "\n",
    "We will use class balancing techniques to see if we can improve the model's performance, especially on the minority class (Ultra). A common technique is oversampling the minority class using the SMOTE (Synthetic Minority Over-sampling Technique) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SMOTE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Aplicar SMOTE para balancear las clases\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m smote \u001b[38;5;241m=\u001b[39m \u001b[43mSMOTE\u001b[49m(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      3\u001b[0m X_train_balanced, y_train_balanced \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X_train, y_train)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Entrenar un nuevo modelo Random Forest con datos balanceados\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SMOTE' is not defined"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train a new Random Forest model with balanced data\n",
    "balanced_rf_model = RandomForestClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=5,\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "balanced_rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_balanced_pred = balanced_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "balanced_test_accuracy = accuracy_score(y_test, y_test_balanced_pred)\n",
    "balanced_conf_matrix = confusion_matrix(y_test, y_test_balanced_pred)\n",
    "balanced_class_report = classification_report(y_test, y_test_balanced_pred)\n",
    "\n",
    "print(f\"Balanced Test Accuracy: {balanced_test_accuracy}\")\n",
    "print(\"Balanced Confusion Matrix:\\n\", balanced_conf_matrix)\n",
    "print(\"Balanced Classification Report:\\n\", balanced_class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations ####\n",
    "\n",
    "SMOTE Application:\n",
    "\n",
    "* SMOTE (Synthetic Minority Over-sampling Technique) was used to balance the classes in the training set (X_train, y_train).\n",
    "* This involves generating synthetic samples of the minority class to ensure both classes have the same number of samples.\n",
    "\n",
    "Model Training with Balanced Data:\n",
    "\n",
    "* A new RandomForestClassifier model was trained using the balanced data generated by SMOTE.\n",
    "* The model's hyperparameters are the same as those in the previous model: max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200, and random_state=42.\n",
    "\n",
    "Predictions and Evaluation on the Test Set:\n",
    "\n",
    "* The model trained with balanced data was used to make predictions on the test set (X_test).\n",
    "* Accuracy: The accuracy of the model on the test set is 0.7636, meaning the model correctly predicts 76.36% of the samples.\n",
    "* Confusion Matrix:\n",
    "    * True Negatives (0 predicted as 0): 363\n",
    "    * False Positives (0 predicted as 1): 83\n",
    "    * False Negatives (1 predicted as 0): 69\n",
    "    * True Positives (1 predicted as 1): 128\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "* Precision:\n",
    "    * Class 0: 0.84\n",
    "    * Class 1: 0.61\n",
    "\n",
    "* Recall:\n",
    "    * Class 0: 0.81\n",
    "    * Class 1: 0.65\n",
    "\n",
    "* F1-score:\n",
    "    * Class 0: 0.83\n",
    "    * Class 1: 0.63\n",
    "\n",
    "* Support:\n",
    "    * Class 0: 446\n",
    "    * Class 1: 197\n",
    "\n",
    "* Averages:\n",
    "    * Macro avg: Precision (0.72), Recall (0.73), F1-score (0.73)\n",
    "    * Weighted avg: Precision (0.77), Recall (0.76), F1-score (0.77)\n",
    "\n",
    "Comparison with the Previous Model:\n",
    "\n",
    "* The accuracy of the model trained with balanced data is lower (76.36%) compared to the previous model trained with unbalanced data (80.72%).\n",
    "* Class 1 (minority):\n",
    "    * Precision, recall, and F1-score for class 1 have improved (0.61, 0.65, and 0.63) compared to the unbalanced model, where these values were lower.\n",
    "* Class 0 (majority):\n",
    "    * Precision, recall, and F1-score for class 0 are slightly lower (0.84, 0.81, and 0.83) compared to the unbalanced model.\n",
    "\n",
    "Implications:\n",
    "\n",
    "* The use of SMOTE has improved the model's performance on the minority class (class 1), at the expense of a slight decrease in overall performance and for the majority class (class 0).\n",
    "* This improvement in the minority class suggests that the model is now more balanced and equitable in terms of performance for both classes, though the trade-off is a decrease in overall accuracy.\n",
    "\n",
    "The use of SMOTE to balance the classes has led to an improvement in performance metrics for the minority class, while the overall model performance has slightly decreased. This approach may be beneficial when it's crucial to improve predictions for the minority class despite a small reduction in overall accuracy.\n",
    "\n",
    "### 5.3. Feature Engineering ###\n",
    "\n",
    "We could attempt to create new features based on the existing ones to improve the model's predictive capacity. Here's a basic example of how to create some additional features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32/3744883429.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['calls_per_day'] = df['calls'] / 30\n",
      "/tmp/ipykernel_32/3744883429.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['minutes_per_call'] = df['minutes'] / df['calls']\n",
      "/tmp/ipykernel_32/3744883429.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['messages_per_day'] = df['messages'] / 30\n",
      "/tmp/ipykernel_32/3744883429.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['mb_per_day'] = df['mb_used'] / 30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 22\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo de Random Forest con las nuevas características\u001b[39;00m\n\u001b[1;32m     14\u001b[0m enhanced_rf_model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[1;32m     15\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     16\u001b[0m     min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m \u001b[43menhanced_rf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Predecir en el conjunto de prueba\u001b[39;00m\n\u001b[1;32m     25\u001b[0m y_test_enhanced_pred \u001b[38;5;241m=\u001b[39m enhanced_rf_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:304\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m     )\n\u001b[0;32m--> 304\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                           \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.9/site-packages/sklearn/base.py:433\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.9/site-packages/sklearn/utils/validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args],\n\u001b[1;32m     68\u001b[0m                                  args[\u001b[38;5;241m-\u001b[39mextra_args:])]\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.9/site-packages/sklearn/utils/validation.py:814\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 814\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m                \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m                \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m                \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m                \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m                \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m                \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multi_output:\n\u001b[1;32m    823\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(y, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m'\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    824\u001b[0m                     ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.9/site-packages/sklearn/utils/validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[1;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args],\n\u001b[1;32m     68\u001b[0m                                  args[\u001b[38;5;241m-\u001b[39mextra_args:])]\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.9/site-packages/sklearn/utils/validation.py:663\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m                          \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name))\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 663\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    667\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m/opt/conda/envs/python3/lib/python3.9/site-packages/sklearn/utils/validation.py:103\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (allow_nan \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(X)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(X)\u001b[38;5;241m.\u001b[39mall()):\n\u001b[1;32m    102\u001b[0m         type_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfinity\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_nan \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNaN, infinity\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    104\u001b[0m                 msg_err\u001b[38;5;241m.\u001b[39mformat\n\u001b[1;32m    105\u001b[0m                 (type_err,\n\u001b[1;32m    106\u001b[0m                  msg_dtype \u001b[38;5;28;01mif\u001b[39;00m msg_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "# Create new features\n",
    "def add_features(df):\n",
    "    df['calls_per_day'] = df['calls'] / 30\n",
    "    df['minutes_per_call'] = df['minutes'] / df['calls']\n",
    "    df['messages_per_day'] = df['messages'] / 30\n",
    "    df['mb_per_day'] = df['mb_used'] / 30\n",
    "    return df\n",
    "\n",
    "X_train = add_features(X_train)\n",
    "X_val = add_features(X_val)\n",
    "X_test = add_features(X_test)\n",
    "\n",
    "# Train the Random Forest model with the new features\n",
    "enhanced_rf_model = RandomForestClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=5,\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "enhanced_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_enhanced_pred = enhanced_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "enhanced_test_accuracy = accuracy_score(y_test, y_test_enhanced_pred)\n",
    "enhanced_conf_matrix = confusion_matrix(y_test, y_test_enhanced_pred)\n",
    "enhanced_class_report = classification_report(y_test, y_test_enhanced_pred)\n",
    "\n",
    "print(f\"Enhanced Test Accuracy: {enhanced_test_accuracy}\")\n",
    "print(\"Enhanced Confusion Matrix:\\n\", enhanced_conf_matrix)\n",
    "print(\"Enhanced Classification Report:\\n\", enhanced_class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations ####\n",
    "\n",
    "Creation of new features:\n",
    "\n",
    "* New features were added to the datasets X_train, X_val, and X_test:\n",
    "    * calls_per_day: Number of calls divided by 30 (average daily calls).\n",
    "    * minutes_per_call: Number of minutes divided by the number of calls (average minutes per call).\n",
    "    * messages_per_day: Number of messages divided by 30 (average daily messages).\n",
    "    * mb_per_day: Number of megabytes used divided by 30 (average daily data usage).\n",
    "\n",
    "Model training with new features:\n",
    "\n",
    "* A RandomForestClassifier model was trained using the training data with the new features.\n",
    "* The model's hyperparameters are the same as in the previous models: max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200, and random_state=42.\n",
    "\n",
    "Predictions and evaluation on the test set:\n",
    "\n",
    "* The model trained with the new features was used to make predictions on the test set (X_test).\n",
    "* Accuracy: The accuracy of the model on the test set is 0.8103, meaning the model correctly predicts 81.03% of the samples.\n",
    "* Confusion matrix:\n",
    "    * True negatives (0 predicted as 0): 412\n",
    "    * False positives (0 predicted as 1): 34\n",
    "    * False negatives (1 predicted as 0): 88\n",
    "    * True positives (1 predicted as 1): 109\n",
    "\n",
    "Classification report:\n",
    "\n",
    "* Precision:\n",
    "    * Class 0: 0.82\n",
    "    * Class 1: 0.76\n",
    "* Recall:\n",
    "    * Class 0: 0.92\n",
    "    * Class 1: 0.55\n",
    "* F1-score:\n",
    "    * Class 0: 0.87\n",
    "    * Class 1: 0.64\n",
    "* Support:\n",
    "    * Class 0: 446\n",
    "    * Class 1: 197\n",
    "\n",
    "Averages:\n",
    "* Macro avg: Precision (0.79), Recall (0.74), F1-score (0.76)\n",
    "* Weighted avg: Precision (0.81), Recall (0.81), F1-score (0.80)\n",
    "\n",
    "Comparison with the previous model:\n",
    "\n",
    "* The accuracy of the improved model (81.03%) is slightly higher than the original model (80.72%).\n",
    "* Class 1 (minority):\n",
    "    * Precision and F1-score for class 1 have slightly improved (precision of 0.76 vs 0.76 and F1-score of 0.64 vs 0.64), but recall remained the same at 0.55.\n",
    "* Class 0 (majority):\n",
    "    * Precision and F1-score for class 0 are the same (0.82 and 0.87, respectively), and recall slightly improved (0.92 vs 0.92).\n",
    "\n",
    "Implications:\n",
    "\n",
    "* The addition of new features has slightly improved the model's performance in terms of accuracy.\n",
    "* The improved model is more balanced and shows minor but notable improvements in performance metrics, especially for class 1.\n",
    "\n",
    "The creation of new features and their integration into the model has led to a slight improvement in overall accuracy and some performance metrics for the minority class. This suggests that the new features provide useful information that the model can use to make more accurate predictions.\n",
    "\n",
    "## 6. Analysis of Results  <a id='analysis_of_results'></a>\n",
    "1. Sanity Check\n",
    "\n",
    "* Random Data Test Accuracy: 0.306\n",
    "This result confirms that the model is not overfitting and works correctly by not achieving a high accuracy with random data.\n",
    "\n",
    "2. Class Balancing with SMOTE\n",
    "\n",
    "- Precision and recall for the Ultra class (1) improved slightly with balancing, but the overall accuracy decreased compared to the original model.\n",
    "\n",
    "3. Feature Engineering\n",
    "\n",
    "- Precision and recall for the Smart class (0) remain high, and the model's accuracy improved slightly with the new features, matching the best original model.\n",
    "\n",
    "## Observations ##\n",
    "\n",
    "- **Model with Feature Engineering**: This approach provided the best results in terms of overall accuracy and balanced performance between both classes. Precision reached 0.81, exceeding the required threshold of 0.75.\n",
    "- **Balanced Model with SMOTE**: Although it improved the ability to detect the Ultra class (1), the overall accuracy was slightly lower.\n",
    "\n",
    "## 7. Final Steps  <a id='final_steps'></a>\n",
    "\n",
    "1. **Final Model**: Use the model with feature engineering as your final model, as it showed the best overall performance.\n",
    "2. **Documentation and Presentation**: Document the process and results, including graphs and visualizations to show the improvements achieved.\n",
    "3. **Implementation**: Consider implementing this model in a production environment and monitoring its performance on real data for continuous adjustments.\n",
    "\n",
    "## General Conclusion ##\n",
    "\n",
    "The analysis and evaluation of different models and feature enhancement techniques have led to an optimized Random Forest model for classifying the target is_ultra. Below are the key conclusions:\n",
    "\n",
    "Data Split:\n",
    "\n",
    "* The data was split into training, validation, and test sets with a 60%, 20%, 20% ratio, resulting in the following shapes: training (1928, 4), validation (643, 4), and test (643, 4).\n",
    "\n",
    "Initial Model Evaluation:\n",
    "\n",
    "* Three basic models were evaluated: Logistic Regression, Random Forest, and SVM.\n",
    "* The Random Forest model showed the best performance on the validation set with an accuracy of 80.09%, followed by SVM and Logistic Regression with accuracies of 75.27% and 74.96%, respectively.\n",
    "\n",
    "Hyperparameter Optimization:\n",
    "\n",
    "* Using GridSearchCV, the best hyperparameters for the Random Forest model were found: max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200.\n",
    "* The optimized model achieved an accuracy of 81.02% on the validation set.\n",
    "\n",
    "Optimized Model Evaluation:\n",
    "\n",
    "* The optimized Random Forest model showed an accuracy of 80.72% on the test set.\n",
    "* The confusion matrix and classification report revealed that the model had good precision and F1-score for the majority class (0), but a lower performance for the minority class (1).\n",
    "\n",
    "Evaluation with Random Data:\n",
    "\n",
    "* When evaluated with random data, the accuracy dropped significantly to 30.64%, confirming that the model has learned specific patterns from the original dataset and is not just guessing.\n",
    "\n",
    "Application of SMOTE for Class Balancing:\n",
    "\n",
    "* By applying SMOTE to balance the classes in the training set, the accuracy on the test set was 76.36%.\n",
    "* The confusion matrix and classification report showed improvements in recall for the minority class (1), though the overall accuracy slightly decreased.\n",
    "\n",
    "Creation of New Features:\n",
    "\n",
    "* Derived features (calls_per_day, minutes_per_call, messages_per_day, mb_per_day) were added to the datasets.\n",
    "* The Random Forest model trained with these new features achieved an accuracy of 81.03% on the test set.\n",
    "* The improved performance suggests that the new features provide additional useful information for classification.\n",
    "\n",
    "Implications and Recommendations\n",
    "\n",
    "* Model Selection: The Random Forest model was consistently superior compared to Logistic Regression and SVM, especially when hyperparameters were optimized.\n",
    "* Feature Engineering: Creating new features improved model performance, highlighting the importance of feature engineering in data analysis.\n",
    "* Class Imbalance: Techniques like SMOTE can help improve performance for minority classes, though they may slightly affect overall accuracy.\n",
    "* Generalization: Results with random data confirm that the model is learning meaningful patterns and not just memorizing the training data.\n",
    "\n",
    "* Final Model: The Random Forest model with feature engineering provided the best results in terms of overall accuracy and a balanced performance between both classes, achieving a precision of 0.81. This model should be considered for final implementation.\n",
    "* Documentation and Presentation: It is important to document the process and results, including graphs and visualizations to illustrate the improvements achieved.\n",
    "* Implementation and Monitoring: Consider implementing this model in a production environment and monitoring its performance on real data for continuous adjustments.\n",
    "\n",
    "The combination of hyperparameter optimization, class balancing, and feature creation resulted in a robust and accurate Random Forest model for classifying the target is_ultra."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 5825,
    "start_time": "2024-08-07T15:01:15.579Z"
   },
   {
    "duration": 0,
    "start_time": "2024-08-07T15:01:21.408Z"
   },
   {
    "duration": 0,
    "start_time": "2024-08-07T15:01:21.409Z"
   },
   {
    "duration": 0,
    "start_time": "2024-08-07T15:01:21.411Z"
   },
   {
    "duration": 32,
    "start_time": "2024-08-07T15:01:21.716Z"
   },
   {
    "duration": 31,
    "start_time": "2024-08-07T15:01:23.431Z"
   },
   {
    "duration": 34,
    "start_time": "2024-08-07T15:01:25.055Z"
   },
   {
    "duration": 723,
    "start_time": "2024-08-07T15:02:42.441Z"
   },
   {
    "duration": 34,
    "start_time": "2024-08-07T15:03:07.815Z"
   },
   {
    "duration": 127,
    "start_time": "2024-08-07T15:03:11.993Z"
   },
   {
    "duration": 40,
    "start_time": "2024-08-07T15:04:47.633Z"
   },
   {
    "duration": 7,
    "start_time": "2024-08-07T15:04:50.462Z"
   },
   {
    "duration": 7,
    "start_time": "2024-08-07T15:04:51.224Z"
   },
   {
    "duration": 16,
    "start_time": "2024-08-07T15:04:57.593Z"
   },
   {
    "duration": 796,
    "start_time": "2024-08-07T15:05:03.425Z"
   },
   {
    "duration": 77063,
    "start_time": "2024-08-07T15:09:50.519Z"
   },
   {
    "duration": 0,
    "start_time": "2024-08-07T15:11:07.595Z"
   },
   {
    "duration": 0,
    "start_time": "2024-08-07T15:11:07.597Z"
   },
   {
    "duration": 0,
    "start_time": "2024-08-07T15:11:07.599Z"
   },
   {
    "duration": 0,
    "start_time": "2024-08-07T15:11:07.602Z"
   },
   {
    "duration": 0,
    "start_time": "2024-08-07T15:11:07.605Z"
   },
   {
    "duration": 0,
    "start_time": "2024-08-07T15:11:07.608Z"
   },
   {
    "duration": 32,
    "start_time": "2024-08-07T15:11:27.262Z"
   },
   {
    "duration": 35,
    "start_time": "2024-08-07T15:11:32.465Z"
   },
   {
    "duration": 7,
    "start_time": "2024-08-07T15:11:35.985Z"
   },
   {
    "duration": 7,
    "start_time": "2024-08-07T15:11:37.231Z"
   },
   {
    "duration": 15,
    "start_time": "2024-08-07T15:11:44.123Z"
   },
   {
    "duration": 789,
    "start_time": "2024-08-07T15:11:51.213Z"
   },
   {
    "duration": 140681,
    "start_time": "2024-08-07T15:12:08.689Z"
   },
   {
    "duration": 905,
    "start_time": "2024-08-07T15:14:35.139Z"
   },
   {
    "duration": 29,
    "start_time": "2024-08-07T15:14:41.945Z"
   },
   {
    "duration": 33,
    "start_time": "2024-08-07T15:14:52.707Z"
   },
   {
    "duration": 270,
    "start_time": "2024-08-07T15:15:04.227Z"
   },
   {
    "duration": 245,
    "start_time": "2024-08-07T15:32:06.579Z"
   },
   {
    "duration": 889,
    "start_time": "2024-08-07T15:33:53.039Z"
   },
   {
    "duration": 879,
    "start_time": "2024-08-07T15:39:35.154Z"
   },
   {
    "duration": 870,
    "start_time": "2024-08-07T15:40:24.524Z"
   },
   {
    "duration": 33,
    "start_time": "2024-08-07T15:46:41.941Z"
   },
   {
    "duration": 30,
    "start_time": "2024-08-07T15:46:47.542Z"
   },
   {
    "duration": 7,
    "start_time": "2024-08-07T15:46:50.639Z"
   },
   {
    "duration": 9,
    "start_time": "2024-08-07T15:46:51.498Z"
   },
   {
    "duration": 16,
    "start_time": "2024-08-07T15:46:55.504Z"
   },
   {
    "duration": 791,
    "start_time": "2024-08-07T15:47:01.000Z"
   },
   {
    "duration": 141032,
    "start_time": "2024-08-07T15:47:08.228Z"
   },
   {
    "duration": 905,
    "start_time": "2024-08-07T15:49:41.007Z"
   },
   {
    "duration": 29,
    "start_time": "2024-08-07T15:49:52.947Z"
   },
   {
    "duration": 28,
    "start_time": "2024-08-07T15:50:03.340Z"
   },
   {
    "duration": 162,
    "start_time": "2024-08-07T15:50:13.931Z"
   },
   {
    "duration": 7,
    "start_time": "2024-08-07T15:50:17.602Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
